{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZjIcyWSY5MW"
      },
      "source": [
        "<h1 align=\"center\">neural waveshaping synthesis</h1>\n",
        "<h4 align=\"center\">real-time neural audio synthesis in the waveform domain</h4>\n",
        "\n",
        "<div align=\"center\">\n",
        "<h4>\n",
        "    <a href=\"https://benhayes.net/assets/pdf/nws_arxiv.pdf\" target=\"_blank\">paper</a> •\n",
        "        <a href=\"https://benhayes.net/projects/nws/\" target=\"_blank\">website</a> • \n",
        "        <a href=\"https://github.com/ben-hayes/neural-waveshaping-synthesis/\" target=\"_blank\">github</a> • \n",
        "        <a href=\"https://benhayes.net/projects/nws/#audio-examples\">audio</a>\n",
        "    </h4>\n",
        "    <p>\n",
        "    by <em>Ben Hayes, Charalampos Saitis, György Fazekas</em>\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<p>\n",
        "This Google Colab notebook is a supplement to our ISMIR 2021 paper, <i>Neural Waveshaping Synthesis</i>.\n",
        "By running the code in the following cells, you can use one of our pretrained models to perform timbre transfer from any monophonic audio to flute, trumpet, or violin!\n",
        "</p>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://benhayes.net/assets/img/newt_shapers.png\" /></p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSGz_-2IOMIr"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "zoJjEMHQuviu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#@title Install dependencies\n",
        "#@markdown \n",
        "!rm -rf neural-waveshaping-synthesis\n",
        "!git clone https://github.com/ben-hayes/neural-waveshaping-synthesis.git\n",
        "%cd neural-waveshaping-synthesis\n",
        "!pip install youtube-dl pytorch-lightning auraloss==0.2.1 librosa==0.8.0 torchcrepe==0.0.12 wandb\n",
        "!pip install -q https://github.com/tugstugi/dl-colab-notebooks/archive/colab_utils.zip\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "XRR5iINku3LA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#@title Make imports\n",
        "\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from dl_colab_notebooks.audio import record_audio\n",
        "import gin\n",
        "from google.colab import files\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "import torch\n",
        "\n",
        "from neural_waveshaping_synthesis.data.utils.loudness_extraction import extract_perceptual_loudness\n",
        "from neural_waveshaping_synthesis.data.utils.mfcc_extraction import extract_mfcc\n",
        "from neural_waveshaping_synthesis.data.utils.f0_extraction import extract_f0_with_crepe\n",
        "from neural_waveshaping_synthesis.data.utils.preprocess_audio import preprocess_audio, convert_to_float32_audio, make_monophonic, resample_audio\n",
        "from neural_waveshaping_synthesis.models.modules.shaping import FastNEWT\n",
        "from neural_waveshaping_synthesis.models.neural_waveshaping import NeuralWaveshaping\n",
        "\n",
        "try:\n",
        "  gin.constant(\"device\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "except ValueError as err:\n",
        "  pass\n",
        "gin.parse_config_file(\"gin/models/newt.gin\")\n",
        "gin.parse_config_file(\"gin/data/urmp_4second_crepe.gin\")\n",
        "\n",
        "checkpoints = dict(Violin=\"vn\", Flute=\"fl\", Trumpet=\"tpt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "YA_giX9AbvSB"
      },
      "outputs": [],
      "source": [
        "#@title Choose device { run: \"auto\" }\n",
        "#@markdown GPU inference will, of course, be faster. If you're only interested in checking out how the model sounds, we'd recommend using a GPU.\n",
        "#@markdown \n",
        "#@markdown You may want to test out CPU inference to check out the model's speed, with or without the FastNEWT. In which case, uncheck the following box and try out the model on CPU!\n",
        "#@markdown If doing so, we recommend still selecting a GPU runtime to run the CREPE pitch extractor, or alternatively disabling the `use_full_crepe_model` option below.\n",
        "#@markdown You'll also need to re-run the following cells when you change this option.\n",
        "use_gpu = False #@param {\"type\": \"boolean\"}\n",
        "dev_string = \"cuda\" if use_gpu else \"cpu\"\n",
        "device = torch.device(dev_string)\n",
        "# gin.config._CONSTANTS['device'] = dev_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "I_HqDjn-7V2v"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "#@title Load Checkpoint { run: \"auto\" }\n",
        "#@markdown Choose from one of three pretrained checkpoints. In future you will be able to upload your own checkpoints too.\n",
        "selected_checkpoint_name = \"Flute\" #@param [\"Violin\", \"Flute\", \"Trumpet\"]\n",
        "selected_checkpoint = checkpoints[selected_checkpoint_name]\n",
        "\n",
        "checkpoint_path = os.path.join(\n",
        "    \"checkpoints/nws\", selected_checkpoint)\n",
        "model = NeuralWaveshaping.load_from_checkpoint(\n",
        "    os.path.join(checkpoint_path, \"last.ckpt\")).to(device)\n",
        "original_newt = model.newt\n",
        "model.eval()\n",
        "data_mean = np.load(\n",
        "    os.path.join(checkpoint_path, \"data_mean.npy\"))\n",
        "data_std = np.load(\n",
        "    os.path.join(checkpoint_path, \"data_std.npy\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0fzadC0Npiu"
      },
      "source": [
        "# Audio Input\n",
        "\n",
        "You now have a few options for getting source audio into the model.\n",
        "Whichever you choose, monophonic audio will give you best results. Polyphony is likely to result in chaos.\n",
        "\n",
        "You only need to run one of these cells. Whichever one you ran last will be used as the model input. When you're done, jump down to **Prepare Audio** below.\n",
        "\n",
        "To start with, why not jump in with the pre-populated YouTube URL?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "gSihCDrj7clT",
        "outputId": "cd3b1f3f-924d-43b1-ff25-3b57da062b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '*.wav': No such file or directory\n",
            "[youtube] UMWMT2M7CSA: Downloading webpage\n",
            "[youtube] UMWMT2M7CSA: Downloading MPD manifest\n",
            "\u001b[0;31mERROR:\u001b[0m unable to download video data: HTTP Error 403: Forbidden\n",
            "mv: cannot stat '*.wav': No such file or directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a0a050b032f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mv *.wav yt_audio.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwavfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yt_audio.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_float32_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_monophonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstart_in_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_in_seconds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlength_in_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(filename, mmap)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mmmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yt_audio.wav'"
          ]
        }
      ],
      "source": [
        "#@title 1. Get Audio from YouTube\n",
        "\n",
        "#@markdown It's hard to beat the default video link...\n",
        "\n",
        "youtube_url = \"https://www.youtube.com/watch?v=UMWMT2M7CSA\" #@param {type: \"string\"}\n",
        "start_in_seconds = 6.5 #@param {type: \"number\"}\n",
        "length_in_seconds = 20.0 #@param {type: \"number\"}\n",
        "\n",
        "!rm *.wav\n",
        "!youtube-dl --extract-audio --audio-format wav {youtube_url} #-o yt_audio.wav\n",
        "!mv *.wav yt_audio.wav\n",
        "\n",
        "rate, audio = wavfile.read(\"yt_audio.wav\")\n",
        "audio = convert_to_float32_audio(make_monophonic(audio))\n",
        "audio = audio[int(rate * start_in_seconds):int(rate * (start_in_seconds + length_in_seconds))]\n",
        "audio = resample_audio(audio, rate, model.sample_rate)\n",
        "ipd.Audio(audio, rate=model.sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaMg2gA7Rdwv"
      },
      "source": [
        "OR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8w09hkUJOorz"
      },
      "outputs": [],
      "source": [
        "#@title 2. Upload an audio file\n",
        "#@markdown For now, only .wav files are supported.\n",
        "\n",
        "#!rm -rf *.wav\n",
        "#uploaded = files.upload()\n",
        "#file_name = list(uploaded.keys())[0]\n",
        "\n",
        "#rate, audio = wavfile.read(file_name)\n",
        "#audio = convert_to_float32_audio(make_monophonic(audio))\n",
        "#audio = resample_audio(audio, rate, model.sample_rate)\n",
        "#ipd.Audio(audio, rate=model.sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv07B0plRfkX"
      },
      "source": [
        "OR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL2_zxLaRgfn"
      },
      "outputs": [],
      "source": [
        "#@title 3. Record audio\n",
        "#@markdown Try singing or whistling into the microphone and becoming an instrument yourself!\n",
        "\n",
        "#record_seconds = 10 #@param {type: \"number\"}\n",
        "#audio = record_audio(record_seconds, sample_rate=model.sample_rate)\n",
        "#ipd.Audio(audio, rate=model.sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANL8C7JzOGMC"
      },
      "source": [
        "# Prepare Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oxM8OhUR_mJ8"
      },
      "outputs": [],
      "source": [
        "#@title Extract Audio Features\n",
        "#@markdown Here we extract F0 using CREPE and A-weighted loudness.\n",
        "\n",
        "use_full_crepe_model = True #@param {type: \"boolean\"}\n",
        "with torch.no_grad():\n",
        "  f0, confidence = extract_f0_with_crepe(\n",
        "      audio,\n",
        "      full_model=use_full_crepe_model,\n",
        "      maximum_frequency=1000)\n",
        "  loudness = extract_perceptual_loudness(audio)\n",
        "\n",
        "fig, axs = plt.subplots(3, 1, True, False, figsize=(12, 5))\n",
        "axs[0].plot(f0)\n",
        "# axs[0].set_title(\"F0\")\n",
        "axs[0].set_ylabel(\"Frequency (Hz)\")\n",
        "axs[1].plot(confidence)\n",
        "# axs[1].set_title(\"Pitch Confidence\")\n",
        "axs[1].set_ylabel(\"Confidence\")\n",
        "axs[2].plot(loudness)\n",
        "axs[2].set_ylabel(\"Loudness\")\n",
        "\n",
        "axs[2].xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: (x * model.control_hop / model.sample_rate)))\n",
        "axs[2].set_xlabel(\"Time (seconds)\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "30CPJ2CQHJu2"
      },
      "outputs": [],
      "source": [
        "#@title Adjust Control Signals { run: \"auto\" }\n",
        "#@markdown Our source audio might not quite match the characteristics of the training audio, so let's adjust it to fit\n",
        "\n",
        "#@markdown Play with these parameters to shift pitch and loudness into an appropriate range:\n",
        "octave_shift = 1 #@param {type: \"slider\", min: -4, max: 4, step: 1}\n",
        "loudness_scale = 0.5 #@param {type: \"slider\", min: 0, max: 2, step: 0.01}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown These parameters are slightly more experimental:\n",
        "loudness_floor = 0.06 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "loudness_conf_filter = 0 #@param {type: \"slider\", min: 0, max: 0.5, step: 0.001}\n",
        "pitch_conf_filter = 0 #@param {type: \"slider\", min: 0, max: 0.5, step: 0.001}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown These parameters are very experimental but can make some fun wacky sounds:\n",
        "pitch_smoothing = 0 #@param {type: \"slider\", min: 0, max: 100}\n",
        "loudness_smoothing = 0 #@param {type: \"slider\", min: 0, max: 100}\n",
        "\n",
        "with torch.no_grad():\n",
        "  f0_filtered = f0 * (confidence > pitch_conf_filter)\n",
        "  loudness_filtered = loudness * (confidence > loudness_conf_filter)\n",
        "  f0_shifted = f0_filtered * (2 ** octave_shift)\n",
        "  loudness_floored = loudness_filtered * (loudness_filtered > loudness_floor) - loudness_floor\n",
        "  loudness_scaled = loudness_floored * loudness_scale\n",
        "  # loudness = loudness * (confidence > 0.4)\n",
        "  \n",
        "  loud_norm = (loudness_scaled - data_mean[1]) / data_std[1]\n",
        "  \n",
        "  f0_t = torch.tensor(f0_shifted, device=device).float()\n",
        "  loud_norm_t = torch.tensor(loud_norm, device=device).float()\n",
        "\n",
        "  if pitch_smoothing != 0:\n",
        "    f0_t = torch.nn.functional.conv1d(\n",
        "      f0_t.expand(1, 1, -1),\n",
        "      torch.ones(1, 1, pitch_smoothing * 2 + 1, device=device) /\n",
        "        (pitch_smoothing * 2 + 1),\n",
        "      padding=pitch_smoothing\n",
        "    ).squeeze()\n",
        "  f0_norm_t = torch.tensor((f0_t.cpu() - data_mean[0]) / data_std[0], device=device).float()\n",
        "\n",
        "  if loudness_smoothing != 0:\n",
        "    loud_norm_t = torch.nn.functional.conv1d(\n",
        "      loud_norm_t.expand(1, 1, -1),\n",
        "      torch.ones(1, 1, loudness_smoothing * 2 + 1, device=device) /\n",
        "        (loudness_smoothing * 2 + 1),\n",
        "      padding=loudness_smoothing\n",
        "    ).squeeze()\n",
        "  f0_norm_t = torch.tensor((f0_t.cpu() - data_mean[0]) / data_std[0], device=device).float()\n",
        "  \n",
        "  control = torch.stack((f0_norm_t, loud_norm_t), dim=0)\n",
        "\n",
        "fig, axs = plt.subplots(2, 1, True, False, figsize=(12, 5))\n",
        "axs[0].plot(f0_norm_t.cpu())\n",
        "# axs[0].set_title(\"F0\")\n",
        "axs[0].set_ylabel(\"Adjusted Frequency\")\n",
        "axs[1].plot(loud_norm_t.cpu())\n",
        "axs[1].set_ylabel(\"Adjusted Loudness\")\n",
        "\n",
        "axs[1].xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: (x * model.control_hop / model.sample_rate)))\n",
        "axs[1].set_xlabel(\"Time (seconds)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df-4FbaTOImG"
      },
      "source": [
        "# Generation Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qyYnp4cAezxW"
      },
      "outputs": [],
      "source": [
        "#@title Select NEWT version { run: \"auto\" }\n",
        "#@markdown Set the parameter below to decide whether to use the original NEWT\n",
        "#@markdown or the FastNEWT optimisation. For more info see\n",
        "#@markdown [the paper](https://arxiv.org/abs/2107.05050).\n",
        "use_fastnewt = False #@param {\"type\": \"boolean\"}\n",
        "if use_fastnewt:\n",
        "  model.newt = FastNEWT(original_newt)\n",
        "else:\n",
        "  model.newt = original_newt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hBq3VDPz_u9h"
      },
      "outputs": [],
      "source": [
        "#@title Synthesise Audio!\n",
        "#@markdown Finally, run this cell to get some audio from the model.\n",
        "with torch.no_grad():\n",
        "  start_time = time.time()\n",
        "  out = model(f0_t.expand(1, 1, -1), control.unsqueeze(0))\n",
        "  run_time = time.time() - start_time\n",
        "rtf = (audio.shape[-1] / model.sample_rate) / run_time\n",
        "print(\"Audio generated in %.2f seconds. That's %.1f times faster than the real time threshold!\" % (run_time, rtf))\n",
        "ipd.Audio(out.detach().cpu().numpy(), rate=model.sample_rate)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of NEWT Timbre Transfer",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}